{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "fHr3mlT94MmK",
        "Z_-zxRcw2jeO",
        "oo8pgpfpxZSd"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5SZSxEq33RV"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import datetime\n",
        "\n",
        "drive.mount('drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Youtube Scraping"
      ],
      "metadata": {
        "id": "fHr3mlT94MmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install youtube_transcript_api scrapetube"
      ],
      "metadata": {
        "id": "cftXVice4Osk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_lang_code = 'th' # or other LRLs such as hi (hindi), id (indo)"
      ],
      "metadata": {
        "id": "g1-P-F8q40NZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scrapetube\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "import time"
      ],
      "metadata": {
        "id": "LmKL1muw4XYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "channel_category = {\n",
        "    \"TED\": \"Education\",\n",
        "    \"kurzgesagt\": \"Science and Technology\",\n",
        "    #\"NickDiGiovanni\": \"Food\",\n",
        "    \"MrBeast\": \"Entertainment\",\n",
        "    \"TEDEd\": \"Education\",\n",
        "    \"mrnigelng\": \"Comedy\",\n",
        "    \"RetiredWorkingForYou\": \"Vlogging\",\n",
        "    \"Bearhugsk\": \"Vlogging\"\n",
        "}\n",
        "video_per_channel = 300"
      ],
      "metadata": {
        "id": "-ts8Qkoo4ZFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transcript_list_from_channel(username, category,id=\"\", sort_by=\"newest\", limit=10):\n",
        "  print(username)\n",
        "  videos = None\n",
        "\n",
        "  if (id !=\"\"):\n",
        "    videos = scrapetube.get_channel(id,sort_by=sort_by,limit=limit)\n",
        "  else: videos = scrapetube.get_channel(channel_username=username,sort_by=sort_by,limit=limit)\n",
        "\n",
        "  full_transcript = []\n",
        "\n",
        "  for video in videos:\n",
        "    data = []\n",
        "    count = 0\n",
        "    try:\n",
        "\n",
        "      th = YouTubeTranscriptApi.get_transcript(video['videoId'], languages=[target_lang_code])\n",
        "      en = YouTubeTranscriptApi.get_transcript(video['videoId'], languages=['en'])\n",
        "      print(th)\n",
        "      print(len(th), len(en))\n",
        "      if (len(th) != len(en) + 1): continue\n",
        "\n",
        "      for i in range(len(en)):\n",
        "        if (i == 0):\n",
        "          sentence = {\n",
        "              \"en\": en[i]['text'],\n",
        "              \"th\": th[i+1]['text'],\n",
        "              \"category\": category\n",
        "          }\n",
        "          count += 1\n",
        "          data.append(sentence)\n",
        "        else:\n",
        "          previousLastChar = data[count - 1]['en'][-1]\n",
        "          if (previousLastChar != \".\"):\n",
        "            data[count - 1]['en'] += en[i]['text']\n",
        "            data[count - 1]['th'] += th[i+1]['text']\n",
        "          else:\n",
        "            sentence = {\n",
        "              \"en\": en[i]['text'],\n",
        "              \"th\": th[i+1]['text'],\n",
        "              \"category\": category\n",
        "            }\n",
        "            count += 1\n",
        "            data.append(sentence)\n",
        "      full_transcript += data\n",
        "\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      continue\n",
        "  return full_transcript"
      ],
      "metadata": {
        "id": "T_onasof43bE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = []"
      ],
      "metadata": {
        "id": "BK3HABMF1qYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ted_data = get_transcript_list_from_channel(\"TED\",category=\"Education\",sort_by=\"popular\",limit=video_per_channel)\n",
        "kurzgesagt_data = get_transcript_list_from_channel(\"kurzgesagt\",sort_by=\"popular\",category=\"Science and Technology\",limit=video_per_channel)\n",
        "nick_data = get_transcript_list_from_channel(\"NickDiGiovanni\",category=\"Food\",limit=video_per_channel)\n",
        "bearhug_data = get_transcript_list_from_channel(username=\"\",id=\"UCOqODGR-AoOTkxB74sc1Xyw\",category=\"Vlogging\",limit=2)"
      ],
      "metadata": {
        "id": "uKpddAC11rKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = []\n",
        "for channel, category in channel_category.items():\n",
        "  all_data += get_transcript_list_from_channel(channel,category=category,limit=video_per_channel)\n",
        "  time.sleep(300)\n",
        "\n",
        "df = pd.DataFrame.from_records(all_data)\n",
        "display(df)\n",
        "\n",
        "title = datetime.datetime.now().strftime(\"%d%H%M\") + \"_YUP_channels_raw.csv\"\n",
        "\n",
        "df.to_csv(title)\n",
        "!cp \"$title\" \"drive/My Drive/\""
      ],
      "metadata": {
        "id": "-tTdc-JD5htt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame.from_records(all_data)\n",
        "display(df)\n",
        "\n",
        "title = datetime.datetime.now().strftime(\"%d%H%M\") + \"_YUP_channels_raw.csv\"\n",
        "\n",
        "df.to_csv(title)\n",
        "!cp \"$title\" \"drive/My Drive/\""
      ],
      "metadata": {
        "id": "BeNdl9INvOOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean DATA"
      ],
      "metadata": {
        "id": "scl3DecABaPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "your_file = title"
      ],
      "metadata": {
        "id": "bQMD3V8mBiRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patterns = [\n",
        "        (r'[A-Z][A-Z][A-Z]:', ' '),\n",
        "        (r'[A-Z][a-z]+:', ' '),\n",
        "        (r'[A-Z][A-Z]:', ' '),\n",
        "        (r'[A-Z]:', ' '),\n",
        "        (r'[A-Z][A-Z][A-Z] :', ' '),\n",
        "        (r'[A-Z][a-z]+ :', ' '),\n",
        "        (r'[A-Z][A-Z] :', ' '),\n",
        "        (r'[A-Z] :', ' '),\n",
        "        (r'[A-Z][A-Z][A-Z]：', ' '),\n",
        "        (r'[A-Z][A-Z]：', ' '),\n",
        "        (r'[A-Z]：', ' '),\n",
        "        (r\"[^)]*\\）\", \" \"),\n",
        "        (r\"\\([^)]*\\)\", \" \"),\n",
        "        (r\"\\（[^)]*\\）\", \" \"),\n",
        "        (r\"\\.\", \" \"),\n",
        "        (r'[-+:!=\"^\\'\\[\\]\\(\\)]', \" \"),\n",
        "        (r\",\", \", \"),\n",
        "        (r\" ,\", \", \"),\n",
        "        (r\"\\\"\", \"\"),\n",
        "        (r\"\\'\", \"\"),\n",
        "        (r\"\\”\", \"\"),\n",
        "        (r\"\\“\", \"\"),\n",
        "        (r\"  +\", \" \"),\n",
        "        (r\"\\n\", \" \"),\n",
        "\n",
        "        # Can add more\n",
        "]"
      ],
      "metadata": {
        "id": "Xl1kiCCJBh4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def clean_data(column_data):\n",
        "    # Check null\n",
        "    if pd.isnull(column_data):\n",
        "        return column_data\n",
        "\n",
        "    # Convert non-string to string\n",
        "    if not isinstance(column_data, str):\n",
        "        column_data = str(column_data)\n",
        "\n",
        "\n",
        "\n",
        "    for pattern, replacement in patterns:\n",
        "        column_data = re.sub(pattern, replacement, column_data)\n",
        "\n",
        "    return column_data.strip()"
      ],
      "metadata": {
        "id": "i-hr3hSUBh8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(your_file)\n",
        "\n",
        "# clean_data\n",
        "df_cleaned = df.applymap(clean_data)\n",
        "\n",
        "# Print sample\n",
        "display(df_cleaned)\n",
        "\n",
        "# Save\n",
        "\n",
        "title = datetime.datetime.now().strftime(\"%d%H%M\") + \"_YUP_cleaned.csv\"\n",
        "\n",
        "df = pd.DataFrame.from_records(df_cleaned)\n",
        "df.to_csv(title)\n",
        "!cp \"$title\" \"drive/My Drive/\""
      ],
      "metadata": {
        "id": "QnsR0DjDBhvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Translation"
      ],
      "metadata": {
        "id": "Mm11ss-Z4RvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deep-translator"
      ],
      "metadata": {
        "id": "b1PP3LpR4Tpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from deep_translator import GoogleTranslator"
      ],
      "metadata": {
        "id": "rcjgyo4f4ZbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(title)\n",
        "df = df.drop([\"Unnamed: 0\",\"Unnamed: 0.1\"],axis=1)"
      ],
      "metadata": {
        "id": "iw9b853sFTri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# translate\n",
        "\n",
        "def translate(x):\n",
        "  try:\n",
        "    x[\"en-th\"] = GoogleTranslator(source='en', target='th').translate(x[\"en\"])\n",
        "  except:\n",
        "    x[\"en-th\"] = None\n",
        "  return x"
      ],
      "metadata": {
        "id": "JAHyGCCtFGJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.apply(lambda x: translate(x), axis=1)\n",
        "\n",
        "title = datetime.datetime.now().strftime(\"%d%H%M\") + \"_YUP_translated.csv\"\n",
        "\n",
        "df.to_csv(title)\n",
        "!cp \"$title\" \"drive/My Drive/\""
      ],
      "metadata": {
        "id": "lx5jFWbGFeNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"$title\" \"drive/My Drive/\""
      ],
      "metadata": {
        "id": "qUmpxf0s98PU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instruct Mining Experiment"
      ],
      "metadata": {
        "id": "igHRMD8c9Tqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate transformers -U sentencepiece transformers -qq seqeval -qq datasets -qq evaluate bert_score parascore==1.0.5 pynndescent"
      ],
      "metadata": {
        "id": "bMFGIV8-9a-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datasets import load_dataset, load_metric\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "-Ashq6NP9dlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and tokenizer\n",
        "model_checkpoint = \"google/mt5-base\"\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", return_tensors=\"pt\",add_prefix_space=True)"
      ],
      "metadata": {
        "id": "lnjMllul9iH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"pichykh/YUP_Parallel\")['train']\n",
        "dataset"
      ],
      "metadata": {
        "id": "0vgPlJm59jV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample random subsets\n",
        "\n",
        "subset_sizes = 1000\n",
        "subsets = []\n",
        "for i in range(8):\n",
        "  subset = dataset.shuffle(seed=42).select(range(1000))\n",
        "  subsets.append(subset)"
      ],
      "metadata": {
        "id": "dlyOWAJS9knP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "\n",
        "bertscore = load(\"bertscore\")"
      ],
      "metadata": {
        "id": "nufP6gWc9mT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    # In case the model returns more than the prediction logits\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100s in the labels as we can't decode them\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
        "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
        "\n",
        "    result = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang=\"th\")\n",
        "    return {\n",
        "        \"precision\": result[\"precision\"],\n",
        "      }"
      ],
      "metadata": {
        "id": "vS6vAhXb9ns6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from parascore import ParaScorer\n",
        "import pynndescent"
      ],
      "metadata": {
        "id": "gT8YnLcK9p7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Indicators\n",
        "\n",
        "def compute_indicators(input_dataset, result_df, embbeded):\n",
        "  # ParaScorer\n",
        "  scorer = ParaScorer(lang=\"th\", model_type = 'bert-base-uncased')\n",
        "  print(input_dataset['en-th'])\n",
        "  cands = [str(input_str) for input_str in input_dataset['en-th']]\n",
        "  sources = [str(input_str) for input_str in input_dataset['th']]\n",
        "  score = scorer.free_score(cands, sources)\n",
        "\n",
        "  df['para'] = score[0]\n",
        "  # KNN\n",
        "\n",
        "  input_data = [(np.pad(np.array(e), pad_width=(0,32-len(e)), constant_values=0)) for e in embbeded['input_ids']]\n",
        "  input_data = np.array(input_data)\n",
        "\n",
        "  index = pynndescent.NNDescent(input_data)\n",
        "  result_df['knn'] = index.neighbor_graph[1][:, 6]"
      ],
      "metadata": {
        "id": "P2V3Xhwb9qUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments,Seq2SeqTrainer\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "max_length = 32\n",
        "results = []\n",
        "def preprocess_function(dataset):\n",
        "  inputs = [str(input_str) for input_str in dataset['en-th']]\n",
        "  targets = [str(target_str) for target_str in dataset['th']]\n",
        "\n",
        "  model_inputs = tokenizer(\n",
        "        inputs, text_target=targets, max_length=max_length, truncation=True\n",
        "    )\n",
        "  return model_inputs\n",
        "\n",
        "\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    evaluation_strategy=\"no\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=3,\n",
        "    predict_with_generate=True,\n",
        "    output_dir=\"/results\"\n",
        ")\n",
        "\n",
        "for subset in subsets:\n",
        "  split_datasets = subset.train_test_split(train_size=0.9)\n",
        "  split_datasets[\"validation\"] = split_datasets.pop(\"test\")\n",
        "\n",
        "  tokenized_datasets = split_datasets.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=split_datasets[\"train\"].column_names,\n",
        "  )\n",
        "\n",
        "  df = pd.DataFrame()\n",
        "\n",
        "  trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "  )\n",
        "\n",
        "  #finetune\n",
        "  trainer.train()\n",
        "  #df[''] = trainer.evaluate(max_length=max_length)['eval_precision']\n",
        "\n",
        "  df['bertscore'] = trainer.evaluate(max_length=max_length)['eval_precision']\n",
        "  compute_indicators(split_datasets['validation'],df,tokenized_datasets['validation'])\n",
        "\n",
        "  X = df[['knn','para']]\n",
        "  y = df['bertscore']\n",
        "\n",
        "  reg = LinearRegression().fit(X, y)\n",
        "   # Record model performance\n",
        "  r2 = reg.score(X, y)\n",
        "  coeff = reg.coef_\n",
        "  results.append({\n",
        "    'subset': subset,\n",
        "    'r2': r2,\n",
        "    'coeff': coeff\n",
        "  })\n",
        "\n",
        "  del trainer\n",
        "  del tokenized_datasets\n",
        "  del split_datasets\n"
      ],
      "metadata": {
        "id": "_ejGetKJ9ve2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parascorer and KNN appears to not be significant to dataset quality\n",
        "\n",
        "Require more dataset and experiments"
      ],
      "metadata": {
        "id": "iyFBNi-99w_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Augment Dataset (optional)"
      ],
      "metadata": {
        "id": "Z_-zxRcw2jeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install openai"
      ],
      "metadata": {
        "id": "UbjLnq1H2z4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "\n",
        "openai_key = input('openai API key')"
      ],
      "metadata": {
        "id": "A_jvBla94-W-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_paraphrases(api_key, input_csv_path, output_csv_path, num_rows=99, chunk_size=3):\n",
        "    # Setting\n",
        "    openai.api_key = api_key\n",
        "    column_name = \"th\"\n",
        "    num_paraphrases = 2\n",
        "    x = 0\n",
        "    # Load CSV\n",
        "    df = pd.read_csv(input_csv_path)\n",
        "    df = df.head(num_rows).dropna()  # Process only the first 99 rows\n",
        "\n",
        "    # Add new columns for paraphrases\n",
        "    df[\"GPT1\"] = \"\"\n",
        "    df[\"GPT2\"] = \"\"\n",
        "\n",
        "    # Process data in chunks\n",
        "    for chunk_start in range(0, len(df), chunk_size):\n",
        "        chunk_end = min(chunk_start + chunk_size, len(df))\n",
        "        chunk_df = df.iloc[chunk_start:chunk_end]\n",
        "\n",
        "        for index, row in chunk_df.iterrows():\n",
        "            thai_word = row[column_name]\n",
        "\n",
        "            # Prompt\n",
        "            prompt = f'Generate {num_paraphrases} paraphrases with the same meaning for the Thai word \"{thai_word}\" in Thai language without changing any meaning'\n",
        "\n",
        "            # Request\n",
        "            try:\n",
        "                response = openai.Completion.create(\n",
        "                    engine=\"text-davinci-003\",\n",
        "                    prompt=prompt,\n",
        "                    max_tokens=3000,  # Set the MAX tokens\n",
        "                )\n",
        "            except openai.error.RateLimitError as e:\n",
        "                print(f\"Rate limit reached. Waiting for 60 seconds before retrying.\")\n",
        "                time.sleep(60)\n",
        "                response = openai.Completion.create(\n",
        "                    engine=\"text-davinci-003\",\n",
        "                    prompt=prompt,\n",
        "                )\n",
        "\n",
        "            # Get paraphrases\n",
        "            paraphrases = []\n",
        "            if \"choices\" in response and response[\"choices\"]:\n",
        "                paraphrases = [\n",
        "                    phrase.strip()\n",
        "                    for phrase in response[\"choices\"][0][\"text\"].split(\"\\n\")\n",
        "                    if phrase.strip()\n",
        "                ]\n",
        "\n",
        "            # A little bit of cleaning\n",
        "            cleaned_paraphrases = [\n",
        "                re.sub(r\"\\d+\\.\\s*\", \"\", phrase) for phrase in paraphrases\n",
        "            ]\n",
        "\n",
        "            # Save paraphrases in the new columns\n",
        "            if len(cleaned_paraphrases) >= 2:\n",
        "                df.at[index, \"GPT1\"] = cleaned_paraphrases[0]\n",
        "                df.at[index, \"GPT2\"] = cleaned_paraphrases[1]\n",
        "\n",
        "            # Pause for a moment before the next iteration\n",
        "            time.sleep(20) # (RPM): Limit 3\n",
        "            x += 1\n",
        "            print(f\"เสร็จแล้วนิดนึง({x})\")\n",
        "\n",
        "    # Save to CSV\n",
        "    df.to_csv(output_csv_path, index=False)\n",
        "    !cp \"$title\" \"drive/My Drive/\"\n",
        "    print(\"เสร็จแล้วโว้ยยยย\")\n"
      ],
      "metadata": {
        "id": "vRw9FBMI2zsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = openai_key\n",
        "input_csv_path = title\n",
        "title = datetime.datetime.now().strftime(\"%d%H%M\") + \"_YUP_translated_GPT.csv\"\n",
        "output_csv_path = title\n",
        "num_rows = 100"
      ],
      "metadata": {
        "id": "5fwiDSXi2zj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_paraphrases(api_key, input_csv_path, output_csv_path, num_rows=54, chunk_size=3)"
      ],
      "metadata": {
        "id": "5gBxel-33Kic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine tuning"
      ],
      "metadata": {
        "id": "oo8pgpfpxZSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "2nupesqPxbX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, models\n",
        "from sentence_transformers import InputExample\n",
        "from torch.utils.data import DataLoader\n",
        "from sentence_transformers import losses\n",
        "\n",
        "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')"
      ],
      "metadata": {
        "id": "w6Xa1xUKyZUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(title)\n",
        "df = df.drop([\"Unnamed: 0\"],axis = 1) #Drop unnecessary attribute\n",
        "df = df.dropna() #Drop NAN value"
      ],
      "metadata": {
        "id": "zLkNt3pjyZRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mapping education, Science and Technology\n",
        "category_mapping = {'Education': 0, 'Science and Technology': 1}\n",
        "df['category'] = df['category'].map(category_mapping)"
      ],
      "metadata": {
        "id": "eXJbhzBQ6EgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Implement dataset from dataframe to DatasetDict\n",
        "dataset = Dataset.from_pandas(df)\n",
        "dataset_dict = DatasetDict({'train': dataset})"
      ],
      "metadata": {
        "id": "tDjf0L8i6A8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import InputExample\n",
        "train_examples = []\n",
        "train_data = dataset_dict['train']\n",
        "\n",
        "n_examples = dataset_dict['train'].num_rows #Number of train data\n",
        "\n",
        "for i in range(n_examples):\n",
        "  example = train_data[i]\n",
        "  train_examples.append(InputExample(texts=[example['en'], example['th']])) #Append pair en with th\n",
        "  train_examples.append(InputExample(texts=[example['th'], example['en-th']]))  #Append pair th with th that translate from en"
      ],
      "metadata": {
        "id": "v02FPeVZylNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_examples,shuffle = True, batch_size = 64) # Load data by DataLoader\n",
        "train_loss = losses.MultipleNegativesRankingLoss(model=model) #Using loss function"
      ],
      "metadata": {
        "id": "XZJkg6m1ylGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 3\n",
        "warmup_steps = int(len(train_dataloader) * num_epochs * 1) #1 mean 100%"
      ],
      "metadata": {
        "id": "u_70YelwyY53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_objectives=[(train_dataloader, train_loss)],epochs=num_epochs,warmup_steps=warmup_steps) #fit model"
      ],
      "metadata": {
        "id": "MxUeYhRcy7Uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "# load dataset for evaluate\n",
        "Evaluate_dataset = load_dataset(\"Patt/copa_th\")"
      ],
      "metadata": {
        "id": "eQv-XPh36dgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Evaluate_dataset['train'] # Attribute in dataset"
      ],
      "metadata": {
        "id": "0s3wJKj36fzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_eva_examples = []\n",
        "train_eva_data = Evaluate_dataset['train']\n",
        "\n",
        "n_examples = 100 #Using 100 dataset\n",
        "\n",
        "#Append list of example_inside into train_eva_examples\n",
        "\n",
        "for i in range(n_examples):\n",
        "    example = train_eva_data[i]\n",
        "    example_inside = []\n",
        "    example_inside.append(example['premise']) #en\n",
        "    example_inside.append(example['premise_th']) #th\n",
        "    example_inside.append(example['score_premise']) #score similarity\n",
        "    train_eva_examples.append(example_inside) #Append example_inside[i] into train_eva_examples"
      ],
      "metadata": {
        "id": "q_Nm03aF6foX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_eva_examples[0] # This is the characteristic of this variable"
      ],
      "metadata": {
        "id": "l_qlbvW-6fWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# Evalaute our fine-tuned model\n",
        "overall = 0 # the total of difference between our prediction and exact value\n",
        "for j in range(len(train_eva_examples)):\n",
        "    sen_embedding = model.encode(train_eva_examples[j]) #Encode for compare similarity\n",
        "    compare = cosine_similarity(sen_embedding[0].reshape(1,-1),sen_embedding[1].reshape(1,-1))[0][0] #Compare similarity between th and en\n",
        "    score_premise = train_eva_examples[j][2] #Exact value predict expected\n",
        "    different = abs(compare - score_premise) #difference between our prediction and exact value\n",
        "    overall = overall + different\n",
        "    print(f\"This is the result of {j+1} data: {compare} this is predict expected: {score_premise} different around: {different}\\n\")"
      ],
      "metadata": {
        "id": "AXk_hi0r6na6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The total difference between prediction and expected will be: {overall}\")\n",
        "# The total of difference between our prediction and exact value for our YUP dataset"
      ],
      "metadata": {
        "id": "1uCaD1vu6nOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelB = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2') #model when it's still not fine-tune by our dataset"
      ],
      "metadata": {
        "id": "njsG18wL6rsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# Evaluate pre-trained model\n",
        "overall_normal = 0 # the total of difference between our prediction and exact value\n",
        "for j in range(len(train_eva_examples)):\n",
        "    sen_embedding = modelB.encode(train_eva_examples[j]) #Encode for compare similarity\n",
        "    compare = cosine_similarity(sen_embedding[0].reshape(1,-1),sen_embedding[1].reshape(1,-1))[0][0] #Compare similarity between th and en\n",
        "    score_premise = train_eva_examples[j][2] #Exact value predict expected\n",
        "    different = abs(compare - score_premise) #difference between model prediction and exact value\n",
        "    overall_normal = overall_normal + different\n",
        "    print(f\"This is the result of {j+1} data: {compare} this is predict expected: {score_premise} different around: {different}\\n\")"
      ],
      "metadata": {
        "id": "Wd6OF-o26rpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The total difference between prediction and expected will be: {overall_normal}\")\n",
        "# The total of difference between pre-trained model prediction and exact value without fine-tuned with our model"
      ],
      "metadata": {
        "id": "De7Sty9d6rm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "save_path = 'model.pth'\n",
        "torch.save(model.state_dict(), save_path) #Save model that already fine-tune with our dataset"
      ],
      "metadata": {
        "id": "vmAR1JBn6rkA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}